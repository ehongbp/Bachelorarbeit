
%typen sind hier: https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management#BibTeX

% use case media mix
@INPROCEEDINGS{9266735,
  author={Tapiceria, Regina Pia Krizzia M. and Roa, Yuan Therense Mari B. and Metrillo, Golden Kenneth A. and Young, Michael N.},
  booktitle={2020 7th International Conference on Frontiers of Industrial Engineering (ICFIE)}, 
  title={Application of Linear Programming in Optimizing Media Mix for Jollibee Food Corporations}, 
  year={2020},
  volume={},
  number={},
  pages={57-61},
  keywords={Media;Social networking (online);Linear programming;Advertising;TV;Business;Mathematical model;jollibee;advertisement;optimization;linear programming;maximization},
  doi={10.1109/ICFIE50845.2020.9266735}}

% werbeausgabe definition
@misc{statista_werbung,
  title        = {Werbung - Deutschland},
  year         = {n.d.},
  url          = {https://de.statista.com/outlook/amo/werbung/deutschland},
  note         = {Zugriff am 15. Dezember 2024}
}

% media einfluss
@misc{kantar_werbeempfaenglichkeit,
  author       = {Kantar},
  title        = {Entwicklung der Werbeempfänglichkeit von Verbrauchern nach Medienkanal in den Jahren 2012 und 2024 [Graph]},
  year         = {2024},
  month        = {September 4},
  howpublished = {In Statista},
  url          = {https://de.statista.com/statistik/daten/studie/1496419/umfrage/werbeempfaenglichkeit-von-verbrauchern-weltweit-nach-medienkanal/},
  note         = {Zugriff am 15. Dezember 2024}
}

% the element of statistical learning
@book{hastie2009elements,
  title     = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  author    = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
  series    = {Springer Series in Statistics},
  edition   = {Second Edition},
  year      = {2009},
  publisher = {Springer},
  note      = {Corrected 12th printing - Jan 13, 2017},
  isbn      = {978-0-387-84857-0}
}


%indirekt huber loss 
@article{indirekthuber,
    author = {Fan, Jianqing and Li, Quefeng and Wang, Yuyan},
    title = {Estimation of High Dimensional Mean Regression in the Absence of Symmetry and Light Tail Assumptions},
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {79},
    number = {1},
    pages = {247-265},
    year = {2016},
    month = {04},
    abstract = {Data subject to heavy-tailed errors are commonly encountered in various scientific fields. To address this problem, procedures based on quantile regression and least absolute deviation regression have been developed in recent years. These methods essentially estimate the conditional median (or quantile) function. They can be very different from the conditional mean functions, especially when distributions are asymmetric and heteroscedastic. How can we efficiently estimate the mean regression functions in ultrahigh dimensional settings with existence of only the second moment? To solve this problem, we propose a penalized Huber loss with diverging parameter to reduce biases created by the traditional Huber loss. Such a penalized robust approximate (RA) quadratic loss will be called the RA lasso. In the ultrahigh dimensional setting, where the dimensionality can grow exponentially with the sample size, our results reveal that the RA lasso estimator produces a consistent estimator at the same rate as the optimal rate under the light tail situation. We further study the computational convergence of the RA lasso and show that the composite gradient descent algorithm indeed produces a solution that admits the same optimal rate after sufficient iterations. As a by-product, we also establish the concentration inequality for estimating the population mean when there is only the second moment. We compare the RA lasso with other regularized robust estimators based on quantile regression and least absolute deviation regression. Extensive simulation studies demonstrate the satisfactory finite sample performance of the RA lasso.},
    issn = {1369-7412},
    doi = {10.1111/rssb.12166},
    url = {https://doi.org/10.1111/rssb.12166},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/79/1/247/49226901/rssb12166-sup-0001-supinfo.pdf},
}




% huber papier
@article{huberpapier,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2238020},
 abstract = {This paper contains a new approach toward a theory of robust estimation; it treats in detail the asymptotic theory of estimating a location parameter for contaminated normal distributions, and exhibits estimators--intermediaries between sample mean and sample median--that are asymptotically most robust (in a sense to be specified) among all translation invariant estimators. For the general background, see Tukey (1960) (p. 448 ff.) Let x1, ⋯, xn be independent random variables with common distribution function F(t - ξ). The problem is to estimate the location parameter ξ, but with the complication that the prototype distribution F(t) is only approximately known. I shall primarily be concerned with the model of indeterminacy F = (1 - ε)Φ + ε H, where $0 \leqq \epsilon < 1$ is a known number, Φ(t) = (2π)-1/2 ∫t -∞ exp(-1/2s2) ds is the standard normal cumulative and H is an unknown contaminating distribution. This model arises for instance if the observations are assumed to be normal with variance 1, but a fraction ε of them is affected by gross errors. Later on, I shall also consider other models of indeterminacy, e.g., $\sup_t |F(t) - \Phi(t)| \leqq \epsilon$. Some inconvenience is caused by the fact that location and scale parameters are not uniquely determined: in general, for fixed ε, there will be several values of ξ and σ such that $\sup_t|F(t) - \Phi((t - \xi)/\sigma)| \leqq \epsilon$, and similarly for the contaminated case. Although this inherent and unavoidable indeterminacy is small if ε is small and is rather irrelevant for practical purposes, it poses awkward problems for the theory, especially for optimality questions. To remove this difficulty, one may either (i) restrict attention to symmetric distributions, and estimate the location of the center of symmetry (this works for ξ but not for σ); or (ii) one may define the parameter to be estimated in terms of the estimator itself, namely by its asymptotic value for sample size n → ∞; or (iii) one may define the parameters by arbitrarily chosen functionals of the distribution (e.g., by the expectation, or the median of F). All three possibilities have unsatisfactory aspects, and I shall usually choose the variant which is mathematically most convenient. It is interesting to look back to the very origin of the theory of estimation, namely to Gauss and his theory of least squares. Gauss was fully aware that his main reason for assuming an underlying normal distribution and a quadratic loss function was mathematical, i.e., computational, convenience. In later times, this was often forgotten, partly because of the central limit theorem. However, if one wants to be honest, the central limit theorem can at most explain why many distributions occurring in practice are approximately normal. The stress is on the word "approximately." This raises a question which could have been asked already by Gauss, but which was, as far as I know, only raised a few years ago (notably by Tukey): What happens if the true distribution deviates slightly from the assumed normal one? As is now well known, the sample mean then may have a catastrophically bad performance: seemingly quite mild deviations may already explode its variance. Tukey and others proposed several more robust substitutes--trimmed means, Winsorized means, etc.--and explored their performance for a few typical violations of normality. A general theory of robust estimation is still lacking; it is hoped that the present paper will furnish the first few steps toward such a theory. At the core of the method of least squares lies the idea to minimize the sum of the squared "errors," that is, to adjust the unknown parameters such that the sum of the squares of the differences between observed and computed values is minimized. In the simplest case, with which we are concerned here, namely the estimation of a location parameter, one has to minimize the expression ∑i (xi - T)2; this is of course achieved by the sample mean T = ∑i xi/n. I should like to emphasize that no loss function is involved here; I am only describing how the least squares estimator is defined, and neither the underlying family of distributions nor the true value of the parameter to be estimated enters so far. It is quite natural to ask whether one can obtain more robustness by minimizing another function of the errors than the sum of their squares. We shall therefore concentrate our attention to estimators that can be defined by a minimum principle of the form (for a location parameter): T = Tn(x1, ⋯, xn) minimizes ∑i ρ(xi - T), \begin{equation*} \tag{M} where \rho is a non-constant function. \end{equation*} Of course, this definition generalizes at once to more general least squares type problems, where several parameters have to be determined. This class of estimators contains in particular (i) the sample mean (ρ(t) = t2), (ii) the sample median (ρ(t) = |t|), and more generally, (iii) all maximum likelihood estimators (ρ(t) = -log f(t), where f is the assumed density of the untranslated distribution). These (M)-estimators, as I shall call them for short, have rather pleasant asymptotic properties; sufficient conditions for asymptotic normality and an explicit expression for their asymptotic variance will be given. How should one judge the robustness of an estimator Tn(x) = Tn(x1, ⋯, xn)? Since ill effects from contamination are mainly felt for large sample sizes, it seems that one should primarily optimize large sample robustness properties. Therefore, a convenient measure of robustness for asymptotically normal estimators seems to be the supremum of the asymptotic variance (n → ∞) when F ranges over some suitable set of underlying distributions, in particular over the set of all F = (1 - ε)Φ + ε H for fixed ε and symmetric H. On second thought, it turns out that the asymptotic variance is not only easier to handle, but that even for moderate values of n it is a better measure of performance than the actual variance, because (i) the actual variance of an estimator depends very much on the behavior of the tails of H, and the supremum of the actual variance is infinite for any estimator whose value is always contained in the convex hull of the observations. (ii) If an estimator is asymptotically normal, then the important central part of its distribution and confidence intervals for moderate confidence levels can better be approximated in terms of the asymptotic variance than in terms of the actual variance. If we adopt this measure of robustness, and if we restrict attention to (M)-estimators, then it will be shown that the most robust estimator is uniquely determined and corresponds to the following ρ:ρ(t) = 1/2t2 for $|t| < k, \rho(t) = k|t| - \frac{1}{2}k^2$ for |t| ≥ k, with k depending on ε. This estimator is most robust even among all translation invariant estimators. Sample mean (k = ∞) and sample median (k = 0) are limiting cases corresponding to ε = 0 and ε = 1, respectively, and the estimator is closely related and asymptotically equivalent to Winsorizing. I recall the definition of Winsorizing: assume that the observations have been ordered, x1 ≤ x2 ≤ ⋯ ≤ xn, then the statistic T = n-1(gxg + 1 + xg + 1 + xg + 2 + ⋯ + xn - h + hxn - h) is called the Winsorized mean, obtained by Winsorizing the g leftmost and the h rightmost observations. The above most robust (M)-estimators can be described by the same formula, except that in the first and in the last summand, the factors xg + 1 and xn - h have to be replaced by some numbers u, v satisfying xg ≤ u ≤ xg + 1 and xn - h ≤ v ≤ xn - h + 1, respectively; g, h, u and v depend on the sample. In fact, this (M)-estimator is the maximum likelihood estimator corresponding to a unique least favorable distribution F0 with density f0(t) = (1 - ε)(2π)-1/2e-ρ(t). This f0 behaves like a normal density for small t, like an exponential density for large t. At least for me, this was rather surprising--I would have expected an f0 with much heavier tails. This result is a particular case of a more general one that can be stated roughly as follows: Assume that F belongs to some convex set C of distribution functions. Then the most robust (M)-estimator for the set C coincides with the maximum likelihood estimator for the unique F0 ε C which has the smallest Fisher information number I(F) = ∫ (f'/f)2f dt among all F ε C. Miscellaneous related problems will also be treated: the case of non-symmetric contaminating distributions; the most robust estimator for the model of indeterminacy $\sup_t|F(t) - \Phi(t)| \leqq \epsilon$; robust estimation of a scale parameter; how to estimate location, if scale and ε are unknown; numerical computation of the estimators; more general estimators, e.g., minimizing $\sum_{i < j} \rho(x_i - T, x_j - T)$, where ρ is a function of two arguments. Questions of small sample size theory will not be touched in this paper.},
 author = {Peter J. Huber},
 journal = {The Annals of Mathematical Statistics},
 number = {1},
 pages = {73--101},
 publisher = {Institute of Mathematical Statistics},
 title = {Robust Estimation of a Location Parameter},
 urldate = {2024-12-20},
 volume = {35},
 year = {1964}
}


%media jollibee food
@inproceedings{Tapiceria2020,
  author    = {Regina Pia Krizzia M. Tapiceria and Yuan Therense Mari B. Roa and Golden Kenneth A. Metrillo and Michael N. Young},
  title     = {Application of Linear Programming in Optimizing Media Mix for Jollibee Food Corporations},
  booktitle = {Proceedings of the 7th International Conference on Frontiers of Industrial Engineering},
  year      = {2020},
  publisher = {Mapua University},
  address   = {Intramuros Manila},
}

%regression 
@book{frick2021data,
  editor = {Detlev Frick and Andreas Gadatsch and Jens Kaufmann and Birgit Lankes and Christoph Quix and Andreas Schmidt and Uwe Schmitz},
  title = {Data Science: Konzepte, Erfahrungen, Fallstudien und Praxis},
  year = {2021},
  publisher = {Springer Vieweg},
  address = {Wiesbaden},
  isbn = {978-3-658-33402-4},
  doi = {10.1007/978-3-658-33403-1},
  note = {ISBN 978-3-658-33403-1 (eBook)},
  url = {https://doi.org/10.1007/978-3-658-33403-1}
}


%offline media oder so
@article{20190411492848,
  title = {Offline wirkt!},
  author = {Pimpl, Roland},
  year = {2019},
  date = {2019-04-11},
  keywords = {Software, Studie, Mediaplanung, Werbewirkung, Digitales Marketing},
  pages = {4},
  journal = {Horizont},
  subtitle = {Werbewirkung: In der Debatte um digital überhitzte Mediapläne zeigt Group M, dass Offline-Medien tatsächlich oft unterschätzt werden},
  issue = {15},
  issn = {0175-7989},
  url = {https://www.wiso-net.de/document/HOR__20190411492848},
  note = {powered by GENIOS}
}


% youtube übertrifft TV
@article{237097,
  title = {ANZEIGE. Analyse von Marketing-Mix-Modellen zeigt: YouTube ist ein Treiber für Effizienz und Effektivität im Marketing-Mix},
  year = {2022},
  date = {2022-08-25},
  journal = {www.textilwirtschaft.de},
  url = {https://www.wiso-net.de/document/TWNE__237097},
  note = {powered by GENIOS}
}


% media kanal youtube generali oder so 
@article{DasZusammenspielKundenloyalität2022,
  title = {Das Zusammenspiel von Kundenloyalität, E-Commerce Excellence und Omnikanal},
  year = {2022},
  date = {2022-12-05},
  journal = {AssCompact},
  volume = {12},
  pages = {112},
  url = {https://www.wiso-net.de/document/ASSC__122205033},
  note = {powered by GENIOS}
}

%mmm and descriptive analytics
@book{brown2024mastering,
  author    = {Iain Brown},
  title     = {Mastering Marketing Data Science: A Comprehensive Guide for Today's Marketers},
  year      = {2024},
  publisher = {John Wiley \& Sons, Incorporated},
  url       = {https://ebookcentral.proquest.com/lib/nordakademie/detail.action?docID=31309751},
  note      = {Accessed via ProQuest Ebook Central}
}


% mmm or something
@book{akinkunmi2018data,
  author    = {Akinkunmi, Mustapha},
  title     = {Data Mining and Market Intelligence: Implications for Decision Making},
  year      = {2018},
  publisher = {Springer International Publishing AG},
  url       = {https://ebookcentral.proquest.com/lib/nordakademie/detail.action?docID=5377969},
  note      = {Accessed via ProQuest Ebook Central}
}



% Herkunft und definition MMM
@INPROCEEDINGS{MMMdef,
  author={Gujar, Praveen and Paliwal, Gunjan and Panyam, Sriram and Kewalramani, Chhaya},
  booktitle={2024 IEEE Technology and Engineering Management Society (TEMSCON LATAM)}, 
  title={The Evolution of Ads Marketing Mix Modeling (MMM): From Regression Models to AI-Powered Planning for SMBs}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  keywords={Adaptation models;Privacy;Automation;Navigation;Media;Planning;Resource management;Artificial intelligence;Optimization;Business;MMM;Digital Advertising;Advertisement Technology (AdTech);Artificial Intelligence;Budget Planning;SMBs;B2B;SaaS Platforms;Cloud},
  doi={10.1109/TEMSCONLATAM61834.2024.10717768}}

%Huber loss methode
@ARTICLE{huberloss,
  author={Ge, Jian and Li, Han and Wang, Hongpeng and Dong, Haobin and Liu, Huan and Wang, Wenjie and Yuan, Zhiwen and Zhu, Jun and Zhang, Haiyang},
  journal={IEEE Sensors Journal}, 
  title={Aeromagnetic Compensation Algorithm Robust to Outliers of Magnetic Sensor Based on Huber Loss Method}, 
  year={2019},
  volume={19},
  number={14},
  pages={5499-5505},
  keywords={Interference;Magnetic sensors;Robustness;Estimation;Aircraft;Calibration;Aeromagnetic compensation;robustness;ordinary least-squares;Huber loss method;goodness-of-fit},
  doi={10.1109/JSEN.2019.2907398}}


% descriptive analysis
@ARTICLE{1207445,
  author={Ferreira de Oliveira, M.C. and Levkowitz, H.},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={From visual data exploration to visual data mining: a survey}, 
  year={2003},
  volume={9},
  number={3},
  pages={378-394},
  keywords={Data mining;Data visualization;Delta modulation;Terminology;Graphical models;Visual databases;Displays;Feedback;Cognitive science},
  doi={10.1109/TVCG.2003.1207445}}
