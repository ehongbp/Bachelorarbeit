\newpage
\section{Theoretische Grundlagen (ca. 5 Seiten/10)}
\subsection{Deskriptive Analyse}
\label{deskriptiveanalyse}
Die deskriptive Analyse stellt einen wichtigen Schritt im Prozess der Datenwissenschaft im Marketing dar, da sie ein umfassendes Verständnis von historischen Daten vermittelt. Sie ermöglicht es den Marketern, wesentliche Muster, Trends und Beziehungen in den Daten zusammenzufassen, zu visualisieren und zu interpretieren. Somit bildet die deskriptive Analyse Grundlagen für fortgeschrittenere Analysen wie prädiktive und präskriptive Analysen. Die deskriptive Analyse ist essenziell, um Kundenverhalten und -präferenzen zu verstehen, Markttrends und Konkurrenzdynamik zu identifizieren, Marketingeffekte zu evaluieren und Entscheidungsfindung im datengesteuerten Marketing zu unterstützen\cite{brown2024mastering}. \\\\
Deskriptive, prädikative und präskriptive Analysen haben verschiedene Verwendungszwecke und Methoden. Die deskriptive Analyse benutzt die Statistik, Datenaggregation und Visualisierung, um das vergangene Verhalten zusammenzufassen und zu verstehen. Sie wird verwendet in Markttrendanalysen, Kundensegmentierung und Analysen der Verkaufsleistung mithilfe der Reports, Dashboards und Charts. Basierend auf historischen Daten wird die prädiktive Analyse verwendet, um den zukünftigen Verlauf vorherzusagen. Dabei entstehen Vorhersagen und Wahrscheinlichkeitswerte auf Grundlagen von Regressionen, maschinellem Lernen und Zeitreihenanalysen. Sie wird verwendet für die Vorhersage der Kundenabwanderung und Absatzprognose. Zum Schluss wird die präskriptive Analyse verwendet, um Maßnahmen für die Zukunft zu empfehlen. Dabei werden Optimierung, Simulation und Entscheidungsbaum angewandt, um Empfehlungen und Entscheidungsmodelle zu erzeugen. Diese Analyse wird verwendet, um die Preisoptimierung, Kampagnenausrichtung und das Bestandsmanagement zu unterstützen. Die deskriptive Analyse hat ein breiteres Analysenspektrum und ergänzt andere Analysen in effektiven Marketingstrategien\cite[S. 51 ff]{brown2024mastering}. 


\subsection{Einführung in die Regression}
\label{einfuehrungInDieRegression}
Die lineare Regression ist eines der am häufigsten eingesetzten Werkzeuge in der Datenanalyse und im Rahmen der fundamentalen Analysemethoden deckt sie den Bereich Prognose ab \cite{frick2021data}.  \\\\
Regressionsanalysen modellieren Zusammenhänge zwischen unabhängigen und abhängigen Variablen. Die unabhängigen Variablen werden als Eingabe in das Modell eingegeben und die abhängige Variable soll prognostiziert werden. Die lineare Regression setzt voraus, dass ein linearer Zusammenhang zwischen den Variablen besteht. Die lineare Regression kann als Formel ausgedrückt werden in der \autoref{eq:simpleregression} \cite{frick2021data}. 
\begin{equation}
Y = \beta_0 + \beta_1 X + \epsilon \tag{3.1}
\label{eq:simpleregression}
\end{equation}
Sei \verb|X| eine unabhängige Variable und \verb|Y| eine abhängige Variable. Durch diese Gleichung wird eine Regressionsgerade gezogen. Durch die Regressionsanalyse sollen der Steigungsparameter $\beta_1$ und der Achsenabschnitt $\beta_0$ gefunden werden. Die in das Diagramm eingetragenen Punkte heißen Beobachtungen. Das Modell kann genutzt werden, den \verb|y'_i|-Wert für die jeweiligen \verb|x_i|-Werte vorherzusagen und mit dem tatsächlichen \verb|y_i| zu vergleichen. Da in der Regel kein vollständiger linearer Zusammenhang zwischen den Variablen vorliegt, besteht ein Unterschied zwischen manchen Beobachtungen und der Regressionsgerade. Der vertikale Abstand zwischen einer Beobachtung und der Regressionsgerade wird Residuum genannt. Das Residuum wird in der Gleichung durch den Fehlerterm $\epsilon$ (\anf{Epsilon}) ausgedrückt \cite{frick2021data}.  \\\\
Wenn mehrere unabhängige Variablen in das Modell eingegeben werden, ist das ein \ac{MLR}. Die Formel wird erweitert für mehr \verb|x|-Werte. In der \autoref{eq:multilinear} wird die \ac{MLR} beschrieben \cite{frick2021data}. 
\begin{equation}
Y = \beta_0 + \beta_1 X_{1} + \beta_2 X_{2} + \dots + \beta_p X_{p} + \varepsilon, \tag{3.2}
\label{eq:multilinear}
\end{equation}
\verb|p| bezeichnet dabei die Anzahl der unabhängigen Variablen. Das \ac{MLR}-Modell ähnelt dem der linearen Regression und unterstellt ebenfalls einen linearen Zusammenhang \cite{frick2021data}. Im \ac{MMM} können die abhängigen Variablen Preis, digitale Ausgaben, Zeitungs- und Magazinkosten, TV-Kosten etc. sein und die Ausgabe-Variable kann Nachfrage, Marktanteil und Gewinn sein \cite{akinkunmi2018data}.
\\\\
Um die optimalen Schätzwerte in einer klassischen linearen Regression zu ermitteln, wird meistens die Methode der kleinsten Quadrate (engl. \ac{OLS}) oder eine Maximum Likelihood-Schätzung (engl. \ac{MLE}) verwendet. In beiden Fällen wird die Summe der kleinsten Abweichungsquadrate (engl. \ac{SSR}), um für die Daten optimale Modellwerte zu liefern \cite[S. 246]{frick2021data}. 
\subsection{Voraussetzungen der Regression}
Um eine multiple lineare oder lineare Regression einzusetzen, müssen bestimmte Voraussetzungen erfüllt werden \cite{akinkunmi2018data}. 
\begin{itemize}
    \item Die Daten haben prinzipiell einen linearen Zusammenhang
    \item Die Extremwerte werden entfernt
    \item Die unabhängigen Variablen korrelieren nicht miteinander
    \item Varianzen der Residuen sind gleichverteilt
    \item Die Residuen sind unabhängig
\end{itemize}
Diese Voraussetzungen können nach der Modellerstellung mithilfe der grafischen Darstellungen geprüft werden. Solange Auffälligkeiten bestehen, sollen die Daten oder das Modell nicht in der gedachten Form verwendet werden. Allerdings können die Daten, die die Voraussetzungen nicht erfüllen, durch eine Datentransformation verwendet werden. Beispielsweise kann das lineare Modell bei der Wertentwicklung eines Sparkontos nicht eingesetzt werden, da durch den Zinseszinseffekt ein exponentieller Zusammenhang besteht. Wenn die abhängige Variable, der Kontowert, logarithmiert wird, entsteht fast ein perfekter linearer Zusammenhang. Nach der Datentransformation ist zu achten, dass die Datenbasis verändert wird und die Interpretation angepasst werden soll. In dem Fall ist der Koeffizient der unabhängigen Variablen nicht mehr die Steigerung in der Einheit des Kontowertes, sondern eine prozentuale Steigerung. Je nach Transformationsart ist die Interpretation mit Sorgfalt zu überprüfen. Zusammenfassend ist die \ac{MLR} eine grundlegende und wichtige Methode der Datenanalyse und erfordert Aufmerksamkeit, weil die Prognose außerhalb der unbekannten Daten mit Unsicherheit verbunden ist \cite{akinkunmi2018data}. 

\subsection{Methode der kleinsten Quadrate}
\label{methodederkleinstenquadrate}
Die Methode der kleinsten Quadrate, \ac{OLS}, ist die häufigst benutzte Methode, um das lineare Modell zu trainieren. In diesem Ansatz wird der Koeffizient $\beta$ gewählt, um die Summe der quadrierten Residuen (engl. \ac{RSS}) zu minimieren \cite{hastie2009elements}.  
\begin{equation}
RSS(\beta) = \sum_{i=1}^{N} (y_i - x_i^T \beta)^2 \tag{3.3}
\label{eq:RSS}
\end{equation}
Wie im \autoref{einfuehrungInDieRegression} beschrieben, wird die lineare Regression durch diese Formel ausgedrückt \cite{hastie2009elements}:
\begin{equation}
f(X) = \beta_0 + \sum_{j=1}^{p} X_j \beta_j \tag{3.4}
\label{eq:linearOLS}
\end{equation}
Ein Eingabe-Vektor \( X^T = (X_1, X_2, \ldots, X_p) \) ist als Trainingsdaten gegeben und wird in die \autoref{eq:linearOLS} eingesetzt. Dieses Modell hat die Annahme, dass die Regressionsfunktion E(X|Y) linear ist, oder dass das lineare Modell eine vernünftige Schätzung ist. Dabei sind die $\beta_j$ die unbekannten Parameter oder Koeffizienten. Die \( X_j \) können von verschiedenen Quellen kommen. Die \( X_j \) können quantitative Eingaben oder transformierte quantitative Eingaben sein. Transformierte quantitative Eingaben sind beispielsweise logarithmierte, quadrierte oder Wurzel gezogene Werte. Sie können aus einer erweiterten Basis stammen, wie \( X_2 = X_1^2, \quad X_3 = X_1^3,\quad \ldots \) Ebenso können sie kodierte, qualitative Eingaben darstellen, wie durch das Umwandeln der kategorialen Werte in numerische Werte wie 1 und 0. Schließlich können sie Werte sein, bei denen eine Interaktion zwischen den Variablen herrscht, wie \( X_3 = X_1 \cdot X_2 \) \cite{hastie2009elements}.\\\\
Normalerweise wird mit Trainingsdaten wie \( ( (x_1, y_1) \cdots (x_N, y_N) ) \) gearbeitet, um den Parameter $\beta$ zu schätzen. Jeder \( x_i = (x_{i1}, x_{i2}, \ldots, x_{ip})^T \) ist ein Vektor der Merkmalsmessungen für den \(i\) -ten Fall. In der Methode der letzten Quadrate werden Koeffizienten \( \beta \equiv (\beta_0, \beta_1, \ldots, \beta_p)^T \) gewählt, um die \ac{RSS} zu minimieren \cite{hastie2009elements}. 
\begin{equation}
RSS(\beta) = \sum_{i=1}^{N} (y_i - f(x_i))^2 
= \sum_{i=1}^{N} \left( y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j \right)^2
\label{eq:rsshoch2}
\tag{3.4}
\end{equation}
Aus statistischer Sicht ist dieses Kriterium valide, wenn die Beobachtungen \( (x_i, y_i) \) eine unabhängige, zufällige Stichprobe aus der Population darstellen. Auch wenn die \(x_i\)-Werte nicht zufällig sind, bleibt es immer noch gültig, wenn die \(y_i\)-Werte unter der Bedingung der gegebenen Eingaben \(x_i\) bedingt unabhängig sind \cite{hastie2009elements}.  \\\\
Dennoch sagt die \autoref{eq:rsshoch2} die Gültigkeit des Modells nicht aus. Sie liefert lediglich die beste lineare Anpassung an die Daten. Die Methode der kleinsten Quadrate ist intuitiv befriedigend, ohne es in Betracht zu ziehen, wie die Daten zustande kommen. Das Kriterium misst den durchschnittlichen Anpassungsfehler. \\\\
Dabei sei X eine \( N \times (p + 1) \) Matrix, wo jede Zeile ein Eingabe-Vektor ist. Jeder Vektor beginnt mit einer 1, um die $\beta_0$ zu ermöglichen. Sei \(y\) ein \(N\)-Vektor der Ausgaben für die Trainingsdaten. Dann kann die \ac{RSS} als Formel so ausgedrückt werden: 
\begin{equation}
RSS(\beta) = (y - X\beta)^T (y - X\beta).
\label{eq:RSSmatrix}
\tag{3.5}
\end{equation}
Das ist eine quadratische Funktion mit \(p + 1\) Parametern. Es wird nach $\beta$ abgeleitet:
\begin{equation}
\begin{aligned}
\frac{\partial RSS}{\partial \beta} &= -2 X^T (y - X\beta) \\
\frac{\partial^2 RSS}{\partial \beta \partial \beta^T} &= 2 X^T X.
\end{aligned}
\label{eq:RSSableitung}
\tag{3.6}
\end{equation}
Es wird angenommen, dass X einen vollen Spaltenrang hat. \( X^T X \) ist positiv infinit. 
\subsection{Huber-Regression}
\label{huberregression}
Huber hat 1964 das Papier \anf{Robust Estimation of A Location Parameter} mit einem neuen Ansatz für die robuste Schätzung veröffentlicht. Das Papier thematisiert die Schätzung eines Lageparameters in einer kontaminierten Normalverteilung. Damit argumentiert Huber, dass die tatsächliche Verteilung der Daten von der angenommenen Verteilung abweichen könnte und somit ein übles arithmetisches Mittel liefert. Eine leichte Abweichung könnte die Varianz sprengen. Es gab schon Substitute für das arithmetische Mittel wie den getrimmten Mittelwert, Windsorizing etc., aber eine allgemeine Theorie der robusten Statistik fehlte noch. Somit bildete Huber die Grundlagen der robusten Statistik in diesem Papier \cite{huberpapier}. \\\\
Die Huber-Loss-Funktion ist eine Mischung aus quadratischem Verlust für relativ kleine Fehler und absolutem Verlust für relativ große Fehler, während das Grad der Mischung von einem Tuning-Parameter gesteuert wird \cite{indirekthuber}. 
\\\\
Traditionelle Schätzmethoden, wie das arithmetische Mittel, summieren die Abweichungen der Datenpunkte und gewichten sie oft quadratisch. Dies führt dazu, dass Ausreißer besonders stark in das Ergebnis einfließen. Dadurch können die Schätzungen ungenau werden, wenn die Daten stark von der erwarteten Verteilung abweichen. 
\begin{equation}
l_\alpha(x) = 
\begin{cases} 
2\alpha^{-1}|x| - \alpha^{-2} & \text{if } |x| > \alpha^{-1}; \\ 
\frac{1}{x^2} & \text{if } |x| \leq \alpha^{-1}. 
\end{cases}
\tag{3.3}
\label{eq:huber}
\end{equation} 
Die Huber-Loss-Funktion ist für kleine \verb|x|-Werte quadratisch und für große \verb|x|-Werte linear. Der Parameter $\alpha$ steuert die Mischung zwischen linearer und quadratischer Bestrafung. Least Squares und \ac{LAD} können als zwei Extremfälle der Huber-Loss-Funktion betrachtet werden. Abweichend vom traditionellen Huber-Schätzer konvergiert der $\alpha$ gegen 0, um Verzerrungen bei der Schätzung in der mittleren Regressionsfunktion zu reduzieren, wenn die bedingte Verteilung nicht symmetrisch ist. Allerdings darf $\alpha$ nicht zu schnell sinken, um die Robustheit zu erhalten \cite{indirekthuber}.   
 

\iffalse
Erkläre die Huber-Regression als statistische Methode und ihre Anwendung im Marketing.
Diskutiere, warum diese Methode für die Analyse von Media ROAS geeignet ist.
Füge ein einfaches Beispiel oder eine Fallstudie hinzu, um die Anwendung zu veranschaulichen.

bonprix setzt bereits die Huber-Regression im Marketing-Mix-Modell ein. Die Huber-Loss-Methode ist eine Erweiterung von OLS (Engl. \anf{Ordinary Least Squares}), einer Regressionsmethode. Die Huber-Loss-Methode minimiert den Einfluss von Ausreißern, indem sie abhängig von der Größe der Residuen eine von zwei Funktionen verwendet, um eine robuste Regressionslinie zu berechnen \cite{huberloss}. \\\\
Seit 2020 werden die Einflüsse der Faktoren wie Online-Marketing, Katalog, Media, Mail, Rabatt etc. mit der Huber-Loss-Methode in Prozent ausgerechnet. Die Rechnung erfolgt in einem Zeitraum von zwei Jahren. Media hat einen kleinen Anteil von 0,5 \% bis 1,6 \% in dem Modell, und der jeweilige Anteil seiner Unterkanäle wurde noch nicht berechnet.
\fi
