\newpage
\section{Theoretische Grundlagen (ca. 5 Seiten/10)}
\label{TheoretischeGrundlagen}
\subsection{Deskriptive Analyse}
\label{deskriptiveanalyse}
Die deskriptive Analyse stellt einen wichtigen Schritt im Prozess der Datenwissenschaft im Marketing dar, da sie ein umfassendes Verständnis von historischen Daten vermittelt. Sie ermöglicht es den Marketern, wesentliche Muster, Trends und Beziehungen in den Daten zusammenzufassen, zu visualisieren und zu interpretieren. Somit bildet die deskriptive Analyse Grundlagen für fortgeschrittenere Analysen wie prädiktive und präskriptive Analysen. Die deskriptive Analyse ist essenziell, um Kundenverhalten und -präferenzen zu verstehen, Markttrends und Konkurrenzdynamik zu identifizieren, Marketingeffekte zu evaluieren und Entscheidungsfindung im datengesteuerten Marketing zu unterstützen\cite{brown2024mastering}. \\\\
Deskriptive, prädikative und präskriptive Analysen haben verschiedene Verwendungszwecke und Methoden. Die deskriptive Analyse benutzt die Statistik, Datenaggregation und Visualisierung, um das vergangene Verhalten zusammenzufassen und zu verstehen. Sie wird verwendet in Markttrendanalysen, Kundensegmentierung und Analysen der Verkaufsleistung mithilfe der Reports, Dashboards und Charts. Basierend auf historischen Daten wird die prädiktive Analyse verwendet, um den zukünftigen Verlauf vorherzusagen. Dabei entstehen Vorhersagen und Wahrscheinlichkeitswerte auf Grundlagen von Regressionen, maschinellem Lernen und Zeitreihenanalysen. Sie wird verwendet für die Vorhersage der Kundenabwanderung und Absatzprognose. Zum Schluss wird die präskriptive Analyse verwendet, um Maßnahmen für die Zukunft zu empfehlen. Dabei werden Optimierung, Simulation und Entscheidungsbaum angewandt, um Empfehlungen und Entscheidungsmodelle zu erzeugen. Diese Analyse wird verwendet, um die Preisoptimierung, Kampagnenausrichtung und das Bestandsmanagement zu unterstützen. Die deskriptive Analyse hat ein breiteres Analysenspektrum und ergänzt andere Analysen in effektiven Marketingstrategien\cite[S. 51 ff]{brown2024mastering}. 


\subsection{Einführung in die lineare Regression}
\label{einfuehrungInDieRegression}
Die lineare Regression ist eines der am häufigsten eingesetzten Werkzeuge in der Datenanalyse und im Rahmen der fundamentalen Analysemethoden deckt sie den Bereich Prognose ab \cite{frick2021data}.  \\\\
Regressionsanalysen modellieren Zusammenhänge zwischen unabhängigen und abhängigen Variablen. Die unabhängigen Variablen werden als Eingabe in das Modell eingegeben und die abhängige Variable soll prognostiziert werden. Die lineare Regression setzt voraus, dass ein linearer Zusammenhang zwischen den Variablen besteht. Die lineare Regression kann als Formel ausgedrückt werden in der \autoref{eq:simpleregression} \cite{frick2021data}. 
\begin{equation}
Y = \beta_0 + \beta_1 X + \epsilon \tag{3.1}
\label{eq:simpleregression}
\end{equation}
Sei \(X\) eine unabhängige Variable und Y eine abhängige Variable. Durch diese Gleichung wird eine Regressionsgerade gezogen. Die Regressionsanalyse schätzt den Steigungsparameter $\beta_1$ und den Achsenabschnitt $\beta_0$. Die in das Diagramm eingetragenen Punkte heißen Beobachtungen. Das Modell kann genutzt werden, den \(y'_i\)-Wert für die jeweiligen \(x_i\)-Werte vorherzusagen und mit dem tatsächlichen \(y_i\) zu vergleichen. Da in der Regel kein vollständiger linearer Zusammenhang zwischen den Variablen vorliegt, besteht ein Unterschied zwischen manchen Beobachtungen und der Regressionsgerade. Der vertikale Abstand zwischen einer Beobachtung und der Regressionsgerade wird Residuum genannt. Das Residuum stellt eine Schätzung für den Fehlerterm $\epsilon$ (\anf{Epsilon}) dar und wird in der Gleichung berücksichtigt \cite{frick2021data}.  \\\\
Wenn mehrere unabhängige Variablen in das Modell eingegeben werden, ist das ein \ac{MLR}. Die Formel wird erweitert für mehr \(x\)-Werte. In der \autoref{eq:multilinear} wird die \ac{MLR} beschrieben \cite{frick2021data}. 
\begin{equation}
Y = \beta_0 + \beta_1 X_{1} + \beta_2 X_{2} + \dots + \beta_p X_{p} + \varepsilon, \tag{3.2}
\label{eq:multilinear}
\end{equation}
\(p\) bezeichnet dabei die Anzahl der unabhängigen Variablen. Das \ac{MLR}-Modell ähnelt dem der linearen Regression und unterstellt ebenfalls einen linearen Zusammenhang \cite{frick2021data}. Im \ac{MMM} können die abhängigen Variablen Preis, digitale Ausgaben, Zeitungs- und Magazinkosten, TV-Kosten etc. sein und die Ausgabe-Variable kann Nachfrage, Marktanteil und Gewinn sein \cite{akinkunmi2018data}.
\\\\
Um die optimalen Schätzwerte in einer klassischen linearen Regression zu ermitteln, wird meistens die Methode der kleinsten Quadrate (engl. \ac{OLS}) oder eine Maximum Likelihood-Schätzung (engl. \ac{MLE}) verwendet. In beiden Fällen wird die Summe der kleinsten Abweichungsquadrate (engl. \ac{SSR}), um für die Daten optimale Modellwerte zu liefern \cite[S. 246]{frick2021data}. 
\subsection{Einschränkungen der Regression}
\label{einschränkungenderregression}
Bei dem Einsatz einer multiplen linearen oder einer linearen Regression können Probleme auftauchen \cite{james2013introduction}: 
\begin{itemize}
    \item \textbf{Nichtlinearität der Beziehungen zwischen der Antwortvariablen und den Prädiktoren:} Das Modell nimmt an, dass ein linearer Zusammenhang besteht. Wenn das nicht der Fall ist, führt dies zu Fehlschätzungen. 
    \item \textbf{Korrelation der Fehlerterme:} Das Modell setzt voraus, dass die Fehlerterme nicht miteinander korreliert sind.
    \item \textbf{Nicht-konstante Varianz der Fehlerterme:} Die Varianz der Fehlerterme soll konstant bleiben. Wenn sie variiert, kann die Unsicherheit nicht korrekt abgeschätzt werden.
    \item \textbf{Ausreißer:} Einzelne Punkte, die sehr weit entfernt von den meisten Datenpunkten liegen, können das Modell stark verzerren.
    \item \textbf{Datenpunkte mit großer Hebelwirkung:} Einige Punkte mit extremen \(x\)-Werten üben großen Einfluss auf das Modell aus.
    \item \textbf{Multikollinearität:} Wenn zwei Merkmale stark miteinander korreliert sind, kann das Modell keine eindeutigen Effekte der einzelnen Variablen abschätzen.
\end{itemize}
Diese Probleme können nach der Modellerstellung mithilfe der grafischen Darstellungen geprüft werden. Solange Auffälligkeiten bestehen, sollen die Daten oder das Modell nicht in der gedachten Form verwendet werden. Allerdings können die Daten, die die Voraussetzungen nicht erfüllen, durch eine Datentransformation verwendet werden. Beispielsweise kann das lineare Modell bei der Wertentwicklung eines Sparkontos nicht eingesetzt werden, da durch den Zinseszinseffekt ein exponentieller Zusammenhang besteht. Die Logarithmierung des Kontowerts erzeugt einen nahezu linearen Zusammenhang. Nach der Datentransformation ist zu achten, dass die Datenbasis verändert wird und die Interpretation angepasst werden soll. In dem Fall ist der Koeffizient der unabhängigen Variablen nicht mehr die Steigerung in der Einheit des Kontowertes, sondern eine prozentuale Steigerung. Je nach Transformationsart ist die Interpretation mit Sorgfalt zu überprüfen \cite{akinkunmi2018data}. \\\\
Eine Methode, um die Kollinearität aufzudecken, ist die Untersuchung der Korrelationsmatrix der Prädiktoren. Wenn ein hoher absoluter Wert in der Matrix auftritt, deutet es auf ein Paar hoch korrelierter Variablen hin und somit auf ein Kollinearitätsproblem in den Daten. Es ist möglich, dass Kollinearität zwischen mehreren Variablen besteht, ohne dass eine hohe Korrelation auftritt. Diese Situation wird Multikollinearität genannt.\\\\
Statt dem Untersuchen der Korrelationsmatrix kann die Multikollinearität durch den \ac{VIF} aufgedeckt werden. Der \ac{VIF} misst den Einfluss der Kollinearität auf die Varianz von $\hat{\beta_j}$. Er wird berechnet, indem die Varianz von $\hat{\beta_j}$ im vollständigen Modell durch die Varianz von $\hat{\beta_j}$ in einem Modell ohne andere Prädiktoren dividiert wird. Der kleinstmögliche Wert des \ac{VIF} ist 1, was ein vollständiges Fehlen von Kollinearität bedeutet. In der Praxis tritt ein kleiner Anteil an Kollinearität zwischen den Prädiktoren auf. Als Faustregel gilt, ein \ac{VIF}-Wert über 5, spätestens ab 10, weist auf eine problematische Kollinearität hin. \\\\
\begin{equation}
VIF(\hat{\beta}_j) = \frac{1}{1 - R^2_{X_j|X_{-j}}}
\label{vif}
\end{equation}
wobei \( R^2_{X_j|X_{-j}} \) das \( R^2 \) aus einer Regression von \( X_j \) auf alle anderen Prädiktoren ist. Wenn \( R^2_{X_j|X_{-j}} \) nahe bei eins liegt, ist Kollinearität vorhanden, und der VIF wird groß sein.\\\\
Zusammenfassend ist die \ac{MLR} eine grundlegende und wichtige Methode der Datenanalyse und erfordert Aufmerksamkeit, weil die Prognose außerhalb der unbekannten Daten mit Unsicherheit verbunden ist \cite{akinkunmi2018data}. 

\subsection{Methode der kleinsten Quadrate}
\label{methodederkleinstenquadrate}
Die Methode der kleinsten Quadrate, \ac{OLS}, ist die häufigst benutzte Methode, um das lineare Modell zu trainieren. In diesem Ansatz wird der Koeffizient $\beta$ gewählt, um die Summe der quadrierten Residuen (engl. \ac{RSS}) zu minimieren \cite{hastie2009elements}. Die \autoref{eq:RSS} stellt die Summe der quadrierten Abweichungen zwischen den tatsächlichen und den geschätzten Ausgabewerten dar. 
\begin{equation}
RSS(\beta) = \sum_{i=1}^{N} (y_i - x_i^T \beta)^2 
\label{eq:RSS}
\end{equation}
\begin{equation}
f(X) = \beta_0 + \sum_{j=1}^{p} X_j \beta_j 
\label{eq:linearOLS}
\end{equation}
Die \autoref{eq:linearOLS} stellt die lineare Regression dar. Ein Eingabe-Vektor \( X^T = (X_1, X_2, \ldots, X_p) \) ist als Trainingsdaten gegeben und wird in die \autoref{eq:linearOLS} eingesetzt. Dieses Modell hat die Annahme, dass die Regressionsfunktion E(X|Y) linear ist, oder dass das lineare Modell eine vernünftige Schätzung ist. Dabei sind die $\beta_j$ die unbekannten Parameter oder Koeffizienten. Die \( X_j \) können unterschiedliche Datentypen repräsentieren. Die \( X_j \) können quantitative Eingaben oder transformierte quantitative Eingaben sein. Transformierte quantitative Eingaben sind beispielsweise logarithmierte, quadrierte oder Wurzel gezogene Werte. Sie können aus einer erweiterten Basis stammen, wie \( X_2 = X_1^2, \quad X_3 = X_1^3,\quad \ldots \) Ebenso können sie kodierte, qualitative Eingaben darstellen, wie durch das Umwandeln der kategorialen Werte in numerische Werte wie 1 und 0. Schließlich können sie Werte sein, bei denen eine Interaktion zwischen den Variablen herrscht, wie \( X_3 = X_1 \cdot X_2 \) \cite{hastie2009elements}.\\\\
Normalerweise wird mit Trainingsdaten wie \( ( (x_1, y_1) \cdots (x_N, y_N) ) \) gearbeitet, um den Parameter $\beta$ zu schätzen. Jeder \( x_i = (x_{i1}, x_{i2}, \ldots, x_{ip})^T \) ist ein Vektor der Merkmalsmessungen für den \(i\) -ten Fall. In der Methode der letzten Quadrate werden Koeffizienten \( \beta \equiv (\beta_0, \beta_1, \ldots, \beta_p)^T \) gewählt, um die \ac{RSS} zu minimieren \cite{hastie2009elements}. 
\begin{equation}
RSS(\beta) = \sum_{i=1}^{N} (y_i - f(x_i))^2 
= \sum_{i=1}^{N} \left( y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j \right)^2
\label{eq:rsshoch2}
\end{equation}
Aus statistischer Sicht ist dieses Kriterium valide, wenn die Beobachtungen \( (x_i, y_i) \) eine unabhängige, zufällige Stichprobe aus der Population darstellen. Auch wenn die \(x_i\)-Werte nicht zufällig sind, bleibt es immer noch gültig, wenn die \(y_i\)-Werte unter der Bedingung der gegebenen Eingaben \(x_i\) bedingt unabhängig sind \cite{hastie2009elements}.  \\\\
Dennoch sagt die \autoref{eq:rsshoch2} die Gültigkeit des Modells nicht aus. Sie liefert lediglich die beste lineare Anpassung an die Daten. Die Methode der kleinsten Quadrate ist intuitiv befriedigend, ohne es in Betracht zu ziehen, wie die Daten zustande kommen. Das Kriterium misst den durchschnittlichen Anpassungsfehler. \\\\
Dabei sei X eine \( N \times (p + 1) \) Matrix, wo jede Zeile ein Eingabe-Vektor ist. Jeder Vektor beginnt mit einer 1, um die $\beta_0$ zu ermöglichen. Sei \(y\) ein \(N\)-Vektor der Ausgaben für die Trainingsdaten. Dann kann die \ac{RSS} als Formel so ausgedrückt werden: 
\begin{equation}
RSS(\beta) = (y - X\beta)^T (y - X\beta).
\label{eq:RSSmatrix}
\end{equation}
Das ist eine quadratische Funktion mit \(p + 1\) Parametern. Es wird nach $\beta$ abgeleitet:
\begin{equation}
\begin{aligned}
\frac{\partial RSS}{\partial \beta} &= -2 X^T (y - X\beta) \\
\frac{\partial^2 RSS}{\partial \beta \partial \beta^T} &= 2 X^T X.
\end{aligned}
\label{eq:RSSableitung}
\end{equation}
Es wird angenommen, dass X einen vollen Rang \(p\) hat \cite{huber1981robust}. Das bedeutet, dass die Spalten von X linear unabhängig sind. Das heißt, dass X keine Spalte hat, die als Vielfaches oder als lineare Kombination der anderen Spalten dargestellt werden kann.  Somit ist \( X^T X \) positiv definit und dadurch ist die Auflösung nach $\beta$ eindeutig \cite{hastie2009elements}: 
\begin{equation}
\begin{aligned}
0 &= -2 X^T (y - X\beta) \quad  \\
0 &= X^T (y - X\beta) \quad  \\
  &= X^T y - X^T X \beta \\
X^T X \beta &= X^T y \\
\beta &= (X^T X)^{-1} X^T y \\
\hat{\beta} &= (X^T X)^{-1} X^T y \quad \text{($\hat{\beta}$ geschätzter Parameter)}
\end{aligned}
\label{XTXderived}
\end{equation}
Wenn die Spalten von X linear abhängig sind, dann hat X keinen vollen Rang \cite{hastie2009elements}. Wenn X keinen vollen Rang hat, handelt es sich um Kollinearität \cite{Maronna2019Robust}. Dies geschieht, wenn zwei Eingaben perfekt miteinander korrelieren. In diesem Fall ist \(X^TX\) singular und der Koeffizient der kleinsten Quadrate $\hat{\beta}$ ist nicht eindeutig definiert \cite{hastie2009elements}. Nicht eindeutig bedeutet, dass es Vektoren \(\beta_1 \neq \beta_2 \text{ gibt, sodass } X\beta_1 = X\beta_2\). Dies impliziert, dass die Funktion unendlich viele Lösungen für $\hat{\beta}$ hat \cite{Maronna2019Robust}. In der Praxis tritt der Fall unvollständiger Rangbedingungen am häufigsten auf, wenn eine oder mehrere qualitative Eingaben redundant kodiert sind. In der Regel lässt sich die nicht-eindeutige Darstellung beheben, indem die Kodierung der qualitativen Daten angepasst oder redundante Spalten in X entfernt werden \cite{hastie2009elements}. \\\\
Die Matrix \( H = X (X^T X)^{-1} X^T \) in der \autoref{hutmatrix} wird als \anf{Hutmatrix} genannt. Da sie einen \anf{Hut} auf \(y\) setzt \cite{huber1981robust}. 

\begin{equation}
\hat{y} = X \hat{\beta} = X(X^T X)^{-1} X^T y \quad 
\label{hutmatrix}
\end{equation}
Es wird klar, dass \(H\) eine symmetrische \( n \times n \) Projektionsmatrix ist. Das heißt, \( HH = H \): 
\begin{equation}
\begin{aligned}
H H &= \left( X (X^T X)^{-1} X^T \right) \left( X (X^T X)^{-1} X^T \right) \\
&= X (X^T X)^{-1} (X^T X) (X^T X)^{-1} X^T \\
&= X (X^T X)^{-1} X^T \\
&= H
\end{aligned}
\label{eq:idempotent}
\end{equation}
Und dass sie \(p\) Eigenwerte gleich 1 und \( n - p \) Eigenwerte gleich 0 hat \cite{huber1981robust}. Ihre diagonalen Elemente \(h_i\) befriedigen:
\begin{equation}
0 < h_i < 1  \label{eq:hi_condition}
\end{equation}
Und die Spur von H ist p: 
\begin{equation}
\text{tr}(H) = p \label{eq:trace_H}
\end{equation}
\(tr(H)\) ist die Summe der diagonalen Elemente von H. Die diagonalen Elemente enthalten wichtige Informationen. Ein großer \(h_i\)-Wert weist auf eine Warnung hin und bedeutet, dass die \(i\)-te Beobachtung einen entscheidenden, aber nicht überprüfbaren Einfluss haben könnte. Ein h < 0,2 scheint unproblematisch zu sein, ein h-Wert zwischen 0,2 und 0,5 ist riskant. Wenn es in der Kontrolle ist, sollte ein h < 0,5 vermieden werden \cite{huber1981robust}. \\\\
Aus geometrischer Sicht hat \(\hat{y}\) den kürzesten Abstand zu \(y\) aufgrund der Orthogonalität. Die Spaltenvektoren von X werden mit \(x_0, x_1, \ldots, x_p\) bezeichnet. Dabei ist \(x_0\) der Vektor mit konstantem Wert 1. Diese Vektoren bilden einen Unterraum von $\mathbb{R}^N$ auf und werden in die Formel \( RSS(\beta) = \|y - X\beta\|^2 \) eingesetzt, um das Ergebnis zu minimieren. Nach der Minimierung entsteht ein Parameter $\hat{\beta}$, sodass der Residuum-Vektor \(y-\hat{y}\) orthogonal zu diesem Unterraum steht. Daher stellt \(\hat{y}\) die orthogonale Projektion von \(y\) dar. Die \anf{Hut}-Matrix berechnet diese orthogonale Projektion, daher wird sie auch Projektionsmatrix genannt \cite{hastie2009elements}. 
\subsection{Huber-Regression}
\label{huberregression}
Im Jahr 1964 veröffentlichte Huber das Paper \anf{Robust Estimation of A Location Parameter}, in dem er einen neuen Ansatz zur robusten Schätzung vorstellte. Das Papier thematisiert die Schätzung eines Lageparameters in einer kontaminierten Normalverteilung. Huber versucht, mehr Robustheit zu erreichen, indem er eine andere Funktion der Fehler minimiert als die Summe ihrer Quadrate. Damit argumentiert Huber, dass die tatsächliche Verteilung der Daten von der angenommenen Verteilung abweichen könnte und somit ein übles arithmetisches Mittel liefert. Eine leichte Abweichung könnte die Varianz sprengen. Es gab schon Substitute für das arithmetische Mittel wie den getrimmten Mittelwert, Windsorizing etc., aber eine allgemeine Theorie der robusten Statistik fehlte noch. Somit bildete Huber die Grundlagen der robusten Statistik in diesem Papier \cite{huberpapier}. \\\\
Die Huber-Loss-Funktion ist eine Mischung aus quadratischem Verlust für relativ kleine Fehler und absolutem Verlust für relativ große Fehler, während das Grad der Mischung von einem Tuning-Parameter gesteuert wird \cite{indirekthuber}. 
\\\\
Traditionelle Schätzmethoden, wie das arithmetische Mittel, summieren die Abweichungen der Datenpunkte und gewichten sie oft quadratisch. Dies führt dazu, dass Ausreißer besonders stark in das Ergebnis einfließen. Dadurch können die Schätzungen ungenau werden, wenn die Daten stark von der erwarteten Verteilung abweichen.    \\\\
Die Huber-Loss-Funktion ist definiert als:
\begin{equation}
\rho(t) = 
\begin{cases} 
  \frac{1}{2} |t|^2 & \text{for } |t| < k, \\ 
  k |t| - \frac{1}{2} k^2 & \text{for } |t| \geq k 
\end{cases}
\label{eq:huber}
\end{equation}
Die Huber-Loss-Funktion ist für kleine \(x\)-Werte quadratisch und für große \(x\)-Werte linear. Der Parameter $\alpha$ steuert die Mischung zwischen linearer und quadratischer Bestrafung. Die Methode der kleinsten Quadrate und die Methode der kleinsten absoluten Abweichungen (\ac{LAD}) können als zwei Extremfälle der Huber-Loss-Funktion betrachtet werden. Abweichend vom traditionellen Huber-Schätzer konvergiert der $\alpha$ gegen 0, um Verzerrungen bei der Schätzung in der mittleren Regressionsfunktion zu reduzieren, wenn die bedingte Verteilung nicht symmetrisch ist. Allerdings darf $\alpha$ nicht zu schnell sinken, um die Robustheit zu erhalten \cite{indirekthuber}.   
 